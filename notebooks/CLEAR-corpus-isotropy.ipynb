{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c8e055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? \n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # makes figs nicer!\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from numpy import linalg\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "sns.set(style='whitegrid',font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "743023b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count model parameters\n",
    "def count_parameters(model):\n",
    "    \"\"\"credit: https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model\"\"\"\n",
    "    \n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        \n",
    "        # if the param is not trainable, skip it\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        \n",
    "        # otherwise, count it towards your number of params\n",
    "        params = parameter.numel()\n",
    "        total_params += params\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "    return total_params\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## TODO: make this a class\n",
    "\n",
    "## Step 2\n",
    "def pca_normalization(points):\n",
    "    \"\"\"points: (m samples x n dimensions)\"\"\"\n",
    "    \n",
    "    pca = PCA(n_components=len(np.transpose(points)))\n",
    "    points = pca.fit_transform(points)\n",
    "\n",
    "    return np.transpose(points)\n",
    "\n",
    "## Step 3\n",
    "def get_diag_of_cov(points):\n",
    "    \"\"\"points: (n dims x m samples)\"\"\"\n",
    "    \n",
    "    n = np.shape(points)[0]\n",
    "    cov = np.cov(points)\n",
    "    cov_diag = cov[np.diag_indices(n)]\n",
    "\n",
    "    return cov_diag\n",
    "\n",
    "## Step 4\n",
    "def normalize_diagonal(cov_diag):\n",
    "\n",
    "    n = len(cov_diag)\n",
    "    cov_diag_normalized = (cov_diag*np.sqrt(n))/np.linalg.norm(cov_diag)\n",
    "\n",
    "    return cov_diag_normalized\n",
    "\n",
    "## Step 5\n",
    "def get_isotropy_defect(cov_diag_normalized):\n",
    "\n",
    "    n = len(cov_diag_normalized)\n",
    "    iso_diag = np.eye(n)[np.diag_indices(n)]\n",
    "    l2_norm = np.linalg.norm(cov_diag_normalized - iso_diag)\n",
    "    normalization_constant = np.sqrt(2*(n-np.sqrt(n)))\n",
    "    isotropy_defect = l2_norm/normalization_constant\n",
    "\n",
    "    return isotropy_defect\n",
    "\n",
    "## Interlude\n",
    "def get_kdims(isotropy_defect, points): \n",
    "    \n",
    "    n = np.shape(points)[0]\n",
    "    k = ((n-(isotropy_defect**2)*(n-np.sqrt(n)))**2) / n\n",
    "    \n",
    "    return k\n",
    "\n",
    "def get_fraction_dims(k, points):\n",
    "    \n",
    "    n = np.shape(points)[0]\n",
    "    phi = k/n\n",
    "    \n",
    "    return phi\n",
    "\n",
    "## Step 6\n",
    "def get_IsoScore(isotropy_defect, points):\n",
    "\n",
    "    n = np.shape(points)[0]\n",
    "    the_score = ((n-(isotropy_defect**2)*(n-np.sqrt(n)))**2-n)/(n*(n-1))\n",
    "\n",
    "    return the_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be7958",
   "metadata": {},
   "source": [
    "### Load the readability dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55c34b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>GPT_Response</th>\n",
       "      <th>Goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. Scott's dog Smart was so trained, that he ...</td>\n",
       "      <td>Mr. Scott had trained his dog Smart so well th...</td>\n",
       "      <td>easier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr. Scott's dog Smart was so trained, that he ...</td>\n",
       "      <td>Mr. Scott's canine, denominated Smart, exhibit...</td>\n",
       "      <td>harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Impeachment is a process in which an official ...</td>\n",
       "      <td>Impeachment is when an official is charged wit...</td>\n",
       "      <td>easier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Original  \\\n",
       "0  Mr. Scott's dog Smart was so trained, that he ...   \n",
       "1  Mr. Scott's dog Smart was so trained, that he ...   \n",
       "2  Impeachment is a process in which an official ...   \n",
       "\n",
       "                                        GPT_Response    Goal  \n",
       "0  Mr. Scott had trained his dog Smart so well th...  easier  \n",
       "1  Mr. Scott's canine, denominated Smart, exhibit...  harder  \n",
       "2  Impeachment is when an official is charged wit...  easier  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv(\"../data/processed/modified/gpt-4-1106-preview_modified.csv\")\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99e73589",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate the dataset\n",
    "\n",
    "sub_easy = df_all[df_all[\"Goal\"]==\"easier\"]\n",
    "sub_hard = df_all[df_all[\"Goal\"]==\"harder\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e3e46",
   "metadata": {},
   "source": [
    "### Filter for Goal Difficulty, Embed the excerpts, Compute the IsoScore for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca2655df",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"FacebookAI/roberta-base\"#,\n",
    "          #\"openai-community/gpt2\",\n",
    "          #\"allenai/OLMoE-1B-7B-0924\"\n",
    "         ]\n",
    "\n",
    "# Decide which device to allocate models to\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6b2c1",
   "metadata": {},
   "source": [
    "##### results for just the original excerpts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af39fc",
   "metadata": {},
   "source": [
    "### testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7cbc8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Params: 124645632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([236, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sub_easy #grabbed just one of the subset dataframes (the originals are the same for both)\n",
    "EXCERPT_TYPE = \"Original\" #column name from df to grab excerpt from\n",
    "\n",
    "mpath = MODELS[0]\n",
    "\n",
    "# Load model & tokenizer from HuggingFace \n",
    "model = transformers.AutoModel.from_pretrained(mpath,output_hidden_states=True)\n",
    "model.to(DEVICE) #allocate model to desired device\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(mpath)\n",
    "\n",
    "# Get some model details to save later\n",
    "mname = mpath.split(\"/\")[-1]\n",
    "n_layers = model.config.num_hidden_layers\n",
    "n_params = count_parameters(model)\n",
    "\n",
    "# Iterate through data passages\n",
    "results = []\n",
    "\n",
    "row = df.iloc[0]\n",
    "\n",
    "excerpt = row[EXCERPT_TYPE]\n",
    "        \n",
    "# Tokenize excerpt\n",
    "inputs = tokenizer(excerpt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# Run model\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs)\n",
    "    hidden_states = output.hidden_states\n",
    "\n",
    "# Iterate through model layers\n",
    "isotropy = []\n",
    "kdims = []\n",
    "\n",
    "layer = 0\n",
    "\n",
    "# Grab layer-specific embeddings\n",
    "# shape (num tokens x num embed dims)\n",
    "layer_embed = hidden_states[layer][0]\n",
    "\n",
    "layer_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8d9fce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([236, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_embed.cpu().detach().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7620d565",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=768 must be between 0 and min(n_samples, n_features)=236 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/95/7z0h1t9s5tx7km1h1t7t08kw0000gn/T/ipykernel_99712/3934821506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# num samples < embedding dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpca_layer_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcov_diag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_diag_of_cov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_layer_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcov_diag_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_diagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov_diag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/95/7z0h1t9s5tx7km1h1t7t08kw0000gn/T/ipykernel_99712/1469302507.py\u001b[0m in \u001b[0;36mpca_normalization\u001b[0;34m(points)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m'np.ascontiguousarray'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \"\"\"\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"arpack\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"randomized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 )\n\u001b[1;32m    474\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0;34m\"n_components=%r must be between 0 and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;34m\"min(n_samples, n_features)=%r with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=768 must be between 0 and min(n_samples, n_features)=236 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "### ISSUE: can't pca-normalize up to n embedding dimensions when \n",
    "# num samples < embedding dimensions\n",
    "\n",
    "pca_layer_embed = pca_normalization(layer_embed.cpu().detach())\n",
    "cov_diag = get_diag_of_cov(pca_layer_embed)\n",
    "cov_diag_normalized = normalize_diagonal(cov_diag)\n",
    "isotropy_defect = get_isotropy_defect(cov_diag_normalized)\n",
    "k = get_kdims(isotropy_defect, pca_layer_embed)\n",
    "phi = get_fraction_dims(k, pca_layer_embed)\n",
    "isoscore = get_IsoScore(isotropy_defect, pca_layer_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4b2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8cd53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498efbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Designing function - IN PROGRESS\n",
    "\n",
    "df = sub_easy #grabbed just one of the subset dataframes (the originals are the same for both)\n",
    "\n",
    "EXCERPT_TYPE = \"Original\" #column name from df to grab excerpt from\n",
    "\n",
    "for mpath in tqdm(MODELS, colour=\"cornflowerblue\"):\n",
    "    \n",
    "    print(mpath)\n",
    "    \n",
    "    # Load model & tokenizer from HuggingFace \n",
    "    model = transformers.AutoModel.from_pretrained(mpath,output_hidden_states=True)\n",
    "    model.to(DEVICE) #allocate model to desired device\n",
    "    \n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(mpath)\n",
    "    \n",
    "    # Get some model details to save later\n",
    "    mname = mpath.split(\"/\")[-1]\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    n_params = count_parameters(model)\n",
    "    \n",
    "    # Iterate through data passages\n",
    "    results = []\n",
    "    for ix,row in tqdm(df.iterrows(),total=df.shape[0],colour=\"hotpink\"): \n",
    "        \n",
    "        excerpt = row[EXCERPT_TYPE]\n",
    "        \n",
    "        # Tokenize excerpt\n",
    "        inputs = tokenizer(excerpt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        # Run model\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "            hidden_states = output.hidden_states\n",
    "            \n",
    "        # Iterate through model layers\n",
    "        isotropy = []\n",
    "        kdims = []\n",
    "        \n",
    "        for layer in range(n_layers+1): \n",
    "            \n",
    "            # Grab layer-specific embeddings\n",
    "            layer_embed = hidden_states[layer][0]\n",
    "            \n",
    "            # Compute k dimensions uniformly used\n",
    "            \n",
    "            \n",
    "            # Compute the IsoScore (Rudman et al. 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa492b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "526ba859",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example code\n",
    "\n",
    "# random_array_1 = np.random.normal(size=100)\n",
    "# random_array_2 = np.random.normal(size=100)\n",
    "# random_array_3 = np.random.normal(size=100)\n",
    "\n",
    "# # Computing the IsoScore for points sampled from a line (dim=1) in R^3\n",
    "# point_cloud_line = np.array([random_array_1, np.zeros(100), np.zeros(100)])\n",
    "\n",
    "# pca_points = pca_normalization(np.transpose(point_cloud_line))\n",
    "\n",
    "# cov_diag = get_diag_of_cov(pca_points)\n",
    "\n",
    "\n",
    "# cov_diag_normalized = normalize_diagonal(cov_diag)\n",
    "\n",
    "\n",
    "# isotropy_defect = get_isotropy_defect(cov_diag_normalized)\n",
    "\n",
    "# k = get_kdims(isotropy_defect, pca_points)\n",
    "\n",
    "# phi = get_fraction_dims(k, pca_points)\n",
    "\n",
    "# the_score = get_IsoScore(isotropy_defect,np.transpose(point_cloud_line))\n",
    "\n",
    "\n",
    "# # the_score = IsoScore.IsoScore(np.transpose(point_cloud_line))\n",
    "# print(f\"IsoScore for 100 points sampled from this line in R^3 is {the_score}.\")\n",
    "# print(f\"k dimensions used uniformly for 100 points from line is {round(k)}.\")\n",
    "# print(f\"phi fraction dimensions used uniformly for 100 points from line is {phi}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1abb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
